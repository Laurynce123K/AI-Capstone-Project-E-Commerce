{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project Task : WEEK 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Class Imbalance Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline\n",
    "\n",
    "train =  pd.read_csv('train_data.csv')\n",
    "test = pd.read_csv('test_data.csv')\n",
    "test_val = pd.read_csv('test_data_hidden.csv')\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Positive = train[train['sentiment']==\"Positive\"].iloc[:,[5,6,7]]\n",
    "Negative = train[train['sentiment']==\"Negative\"].iloc[:,[5,6,7]]\n",
    "Neutral = train[train['sentiment']==\"Neutral\"].iloc[:,[5,6,7]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Positive['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Negative['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Neutral['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Converting the Reviews as Tf-Idf Score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keeping only those feature that we need for futher exploring \n",
    "data = train[['sentiment','reviews.text']]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using matplotlib to show distribution of reviews sentiment in the data \n",
    "print(data.sentiment.value_counts())\n",
    "data['sentiment'].value_counts().plot(kind='bar')\n",
    "plt.title('Distribution of Reviews sentiment', size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating and applying Preprocessing on the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "wordnet_lematizer = WordNetLemmatizer()\n",
    "tokenizer = RegexpTokenizer(r'[a-z]+')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess(document):\n",
    "    #convert to lowercase\n",
    "    document = document.lower()\n",
    "    #tokenize\n",
    "    words = tokenizer.tokenize(document)\n",
    "    # Removing stopwords\n",
    "    words = [w for w in words if not w in stop_words]\n",
    "    #Lemmatizing\n",
    "    for pos in [wordnet.NOUN , wordnet.VERB , wordnet.ADJ, wordnet.ADV]:\n",
    "        words = [wordnet_lematizer.lemmatize(x,pos) for x in words]\n",
    "    return \" \".join(words)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Processed_Review'] = data['reviews.text'].apply(preprocess)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = data.drop(['reviews.text'],axis=1)\n",
    "data1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating TF-IDF & multinomial Naive Bayes classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TextPreprocessing(data2):\n",
    "    #Remove Punctuation Logic\n",
    "    import string\n",
    "    removePunctuation = [char for char in data2 if char not in string.punctuation]\n",
    "    #Join Chars to form sentences\n",
    "    sentenceWithoutPunctuations = ''.join(removePunctuation)\n",
    "    words = sentenceWithoutPunctuations.split()\n",
    "    #StopwordRemoval\n",
    "    from nltk.corpus import stopwords\n",
    "    removeStopwords = [word for word in words if word.lower() not in stopwords.words('english')]\n",
    "    \n",
    "    return removeStopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1.groupby('sentiment').describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Text preprocessing\n",
    "data1['Processed_Review'].head(2).apply(TextPreprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating Bag of Words\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "BOW = CountVectorizer(analyzer=TextPreprocessing).fit(data1['Processed_Review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(BOW.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_BOW = BOW.transform(data1['Processed_Review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_BOW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer \n",
    "Tfidfdata = TfidfTransformer()\n",
    "Tfidfdata.fit(reviews_BOW)\n",
    "TfidfdataFinal = Tfidfdata.transform(reviews_BOW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TfidfdataFinal.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "model = MultinomialNB()\n",
    "model.fit(TfidfdataFinal,data1['sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputdata = 'very bad i dont like it at all it sucks !'\n",
    "l1 = TextPreprocessing(inputdata)\n",
    "l2 = BOW.transform(l1)\n",
    "l3 = Tfidfdata.transform(l2)\n",
    "prediction = model.predict(l3[0])\n",
    "prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "After running Multinomial Naive Bayes Classifier Everything is classified as positive because of the class imbalance as seen above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tackling Class Imbalance Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating independant and dependant feature\n",
    "columns = data1.columns.tolist()\n",
    "# Filtering the column remove data that we don't want \n",
    "columns = [c for c in columns if c not in ['sentiment']]\n",
    "# store the variable we're predicting \n",
    "target = 'sentiment'\n",
    "# define a random state\n",
    "state = np.random.RandomState(42)\n",
    "X = data1[columns]\n",
    "Y = data1[target]\n",
    "print(X.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data1['sentiment'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Positive.shape, Negative.shape,Neutral.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply Oversampling or Undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RandomOverSampler to handle imbalanced data\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "ros = RandomOverSampler(random_state=0)\n",
    "X_res,Y_res = ros.fit_resample(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "print(sorted(Counter(Y_res).items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_res.shape,Y_res.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating X output to DataFrame\n",
    "X1 = pd.DataFrame(X_res,columns=['Processed_Review'])\n",
    "# Creating Y output \n",
    "Y1 = pd.DataFrame(Y_res,columns=['sentiment'])\n",
    "# Merging X & Y \n",
    "Final_data = pd.concat([X1,Y1],axis=1)\n",
    "Final_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Final_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = Final_data.sample(frac=0.1,random_state=0)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have to drop these missing values\n",
    "df.dropna(inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Train & Test split the Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting data in trainning set and validation\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,Y_train,Y_test = train_test_split(df['Processed_Review'],df['sentiment'],test_size=0.2,random_state=0)\n",
    "print(X_train.shape,X_test.shape,Y_train.shape,Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Show a review in a training set: \\n',X_train.iloc[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup  \n",
    "import re\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.stem import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanText(raw_text, remove_stopwords=False, stemming=False, split_text=False,):\n",
    "    text = BeautifulSoup(raw_text, 'lxml').get_text()  # Remove html\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", text)  # Remove non-character\n",
    "    words = letters_only.lower().split() # Convert to lower case \n",
    "    \n",
    "    if remove_stopwords: # Remove stopword\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        words = [w for w in words if not w in stops]\n",
    "        \n",
    "    if stemming==True: # Stemming\n",
    "        stemmer = SnowballStemmer('english') \n",
    "        words = [stemmer.stem(w) for w in words]\n",
    "        \n",
    "    if split_text==True:  # Split text\n",
    "        return (words)\n",
    "    \n",
    "    return( \" \".join(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess text data in validation set and training set\n",
    "X_train_cleaned = []\n",
    "X_test_cleaned = []\n",
    "\n",
    "for d in X_train:\n",
    "    X_train_cleaned.append(cleanText(d))\n",
    "print('Show a cleaned review in training set :\\n',X_train_cleaned[10])\n",
    "for d in X_test:\n",
    "    X_test_cleaned.append(cleanText(d))\n",
    "print('Show a cleaned review in validation set :\\n',X_test_cleaned[10])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit and transform the training data to a document-term matrix using CountVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "countvect = CountVectorizer()\n",
    "X_train_countvect = countvect.fit_transform(X_train_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of features:\",len(countvect.get_feature_names()))\n",
    "print(\"Show some features : \\n\",countvect.get_feature_names()[::1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "mnb = MultinomialNB()\n",
    "mnb.fit(X_train_countvect,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score,classification_report,confusion_matrix,roc_auc_score\n",
    "def modelEvaluation(predictions):\n",
    "    print('Accuracy on validation set :{:.4f}'.format(accuracy_score(Y_test,predictions)))\n",
    "    print('\\nClassification report :\\n',classification_report(Y_test,predictions))\n",
    "    print('\\nConfusion matrix :\\n',confusion_matrix(Y_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on validation set\n",
    "X_test_countvect = countvect.transform(X_test_cleaned)\n",
    "predictions = mnb.predict(X_test_countvect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelEvaluation(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tfidf Vectorizer with Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting and transforming the training data to a document-term matrix with TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "Tfidf = TfidfVectorizer(min_df=5)\n",
    "X_train_Tfidf = Tfidf.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of features:\",len(Tfidf.get_feature_names()))\n",
    "print(\"Show some feature names :\", Tfidf.get_feature_names()[::1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train_Tfidf,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating on Validation set \n",
    "predictions = lr.predict(Tfidf.transform(X_test_cleaned))\n",
    "modelEvaluation(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TfidfVectorizer with Linear SVM by using SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting and Transforming the training data to a document-term using TfidfVectorizer \n",
    "Tfidf = TfidfVectorizer(min_df=5)\n",
    "X_train_Tfidf = Tfidf.fit_transform(X_train)\n",
    "print(\"Number of features:\",len(Tfidf.get_feature_names()))\n",
    "print(\"Show some feature names :\", Tfidf.get_feature_names()[::1000])\n",
    "# SVM\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "clf = SGDClassifier()\n",
    "clf.fit(X_train_Tfidf,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluationg on the Validation set \n",
    "predictions = clf.predict(Tfidf.transform(X_test_cleaned))\n",
    "modelEvaluation(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using XGBoost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost\n",
    "from xgboost import XGBClassifier\n",
    "xgb = XGBClassifier()\n",
    "xgb.fit(X_train_Tfidf,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating on the Validation set \n",
    "predictions = xgb.predict(Tfidf.transform(X_test_cleaned))\n",
    "modelEvaluation(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pipeline and GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building a pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "estimators = [(\"tfidf\", TfidfVectorizer()), (\"lr\", LogisticRegression())]\n",
    "model = Pipeline(estimators)\n",
    "\n",
    "\n",
    "# Grid search\n",
    "params = {\"lr__C\":[0.1, 1, 10], #regularization param of logistic regression\n",
    "          \"tfidf__min_df\": [1, 3], #min count of words \n",
    "          \"tfidf__max_features\": [1000, None], #max features\n",
    "          \"tfidf__ngram_range\": [(1,1), (1,2)], #1-grams or 2-grams\n",
    "          \"tfidf__stop_words\": [None, \"english\"]} #use stopwords or don't\n",
    "\n",
    "grid = GridSearchCV(estimator=model, param_grid=params, scoring=\"accuracy\", n_jobs=-1)\n",
    "grid.fit(X_train_cleaned, Y_train)\n",
    "print(\"The best paramenter set is : \\n\", grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on the validation set \n",
    "predictions = grid.predict(X_test_cleaned)\n",
    "modelEvaluation(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project Task: Week 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Selection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rdc = RandomForestClassifier()\n",
    "rdc.fit(X_train_Tfidf,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating on the validation set \n",
    "predictions = rdc.predict(Tfidf.transform(X_test_cleaned))\n",
    "modelEvaluation(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = Final_data.sample(frac=0.1,random_state=0)\n",
    "# Drop missing values \n",
    "df.dropna(inplace=True)\n",
    "# Convert the sentiments \n",
    "df.sentiment.replace(('Positive','Negative','Neutral'),(1,0,2),inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting data into trainning data and validation \n",
    "X_train,X_test,y_train,y_test = train_test_split(df['Processed_Review'],df['sentiment'],test_size=0.2,random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_words = 20000\n",
    "maxlen = 100\n",
    "batch_size = 32\n",
    "nb_classes = 3\n",
    "epoch = 5\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "# Vectorize X_train and X_test to 2D tensor \n",
    "tokenizer = Tokenizer(num_words=top_words) # converting only tof 20000 in corpus \n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "sequences_train = tokenizer.texts_to_sequences(X_train)\n",
    "sequences_test = tokenizer.texts_to_sequences(X_test)\n",
    "from keras.preprocessing import sequence\n",
    "X_train_seq = sequence.pad_sequences(sequences_train, maxlen=maxlen)\n",
    "X_test_seq = sequence.pad_sequences(sequences_test, maxlen=maxlen)\n",
    "\n",
    "# One-Hot Encoding of y_test and y_train \n",
    "from keras.utils import np_utils\n",
    "y_test_seq = np_utils.to_categorical(y_test,nb_classes)\n",
    "y_train_seq = np_utils.to_categorical(y_train,nb_classes)\n",
    "print('X_train shape :', X_train_seq.shape)\n",
    "print('X_test shape :', X_test_seq.shape)\n",
    "print('y_train shape :', y_train_seq.shape)\n",
    "print('y_test shape :', y_test_seq.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers import LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constructing a Simple LSTM\n",
    "model1 = Sequential()\n",
    "model1.add(Embedding(top_words, 128))\n",
    "model1.add(LSTM(128)) \n",
    "model1.add(Dropout(0.2))\n",
    "model1.add(Dense(nb_classes))\n",
    "model1.add(Activation('softmax'))\n",
    "model1.summary()\n",
    "\n",
    "# Compiling LSTM\n",
    "model1.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model1.fit(X_train_seq, y_train_seq, batch_size=batch_size, epochs=epoch, verbose=1)\n",
    "\n",
    "# Model Evaluation\n",
    "score = model1.evaluate(X_test_seq, y_test_seq, batch_size=batch_size)\n",
    "print('Test loss : {:.4f}'.format(score[0]))\n",
    "print('Test accuracy : {:.4f}'.format(score[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting weight matrix of the embedding layer\n",
    "model1.layers[0].get_weights()[0] # weight matrix of the embedding layer, word-by-dim matrix\n",
    "print(\"Size of weight matrix in the embedding layer : \",model1.layers[0].get_weights()[0].shape)\n",
    "\n",
    "# Getting weight matrix of the hidden layer\n",
    "print(\"Size of weight matrix in the hidden layer : \",model1.layers[1].get_weights()[0].shape) \n",
    "\n",
    "# Getting weight matrix of the output layer\n",
    "#print(\"Size of weight matrix in the output layer : \", model1.layers[2].get_weights()[0].shape) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Topic Modeling "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Latent Dirichet Allocation (LDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk \n",
    "nltk.download('wordnet')\n",
    "\n",
    "doc_complete = data1['Processed_Review'].tolist()\n",
    "doc_clean = [cleanText(doc).split() for doc in doc_complete]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim import corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary =  corpora.Dictionary(doc_clean)\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]\n",
    "print(len(doc_term_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import LdaModel\n",
    "NUM_TOPICS = 9\n",
    "ldamodel = LdaModel(doc_term_matrix,num_topics=NUM_TOPICS,id2word=dictionary,passes=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = ldamodel.show_topics()\n",
    "for topic in topics:\n",
    "    print(topic)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dict = {}\n",
    "for i in range(NUM_TOPICS):\n",
    "    words = ldamodel.show_topic(i, topn = 20)\n",
    "    word_dict[\"Topic # \" + \"{}\".format(i)] = [i[0] for i in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(word_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldamodel.show_topic(0, topn=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a Wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud, STOPWORDS\n",
    "txt = data1[\"Processed_Review\"].values\n",
    "wc = WordCloud(width=200, height=100, background_color=\"white\", stopwords=STOPWORDS).generate(str(txt))\n",
    "fig = plt.figure(figsize=(20,20), facecolor='k', edgecolor='w')\n",
    "plt.imshow(wc, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Displaying Results & Getting Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyLDAvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis.gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Lda_display = pyLDAvis.gensim.prepare(ldamodel, doc_term_matrix, dictionary, sort_topics=False)\n",
    "pyLDAvis.display(Lda_display)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
